#!/usr/bin/env python3
"""
Peaky Mediaè¨˜äº‹ã¾ã¨ã‚è‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒé™å®šç‰ˆï¼‰
https://peaky.co.jp/feed/ ã‹ã‚‰Product Researchã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’å–å¾—ã—ã€AIãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã¾ã¨ã‚è¨˜äº‹ã‚’ç”Ÿæˆ
"""

import os
import asyncio
import feedparser
import requests
from datetime import datetime, timedelta
from dotenv import load_dotenv
import re
import json
from typing import List, Dict
import html

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

class PeakyArticleGenerator:
    def __init__(self):
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
        if not self.anthropic_api_key:
            raise ValueError("ANTHROPIC_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        
        self.peaky_feed_url = "https://peaky.co.jp/feed/"
        self.output_dir = "articles"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(self.output_dir, exist_ok=True)
    
    def fetch_peaky_articles(self) -> List[Dict]:
        """Peaky Mediaã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰Product Researchè¨˜äº‹ã®ã¿ã‚’å–å¾—"""
        print("ğŸ“¡ Peaky Mediaã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­...")
        
        try:
            # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
            feed = feedparser.parse(self.peaky_feed_url)
            
            if feed.bozo:
                print("âš ï¸ RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®è§£æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒç¶šè¡Œã—ã¾ã™")
            
            all_articles = []
            product_research_articles = []
            
            for entry in feed.entries:
                # è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
                article = {
                    'title': entry.title,
                    'link': entry.link,
                    'summary': self._clean_html_content(getattr(entry, 'summary', '')),
                    'published': getattr(entry, 'published', ''),
                    'tags': [tag.term for tag in getattr(entry, 'tags', [])],
                    'category': getattr(entry, 'category', ''),
                    'content': self._extract_content(entry)
                }
                
                all_articles.append(article)
                
                # Product Researchã‚«ãƒ†ã‚´ãƒªã‹ã©ã†ã‹ã‚’åˆ¤å®š
                if self._is_product_research(entry):
                    product_research_articles.append(article)
            
            print(f"âœ… Product Researchè¨˜äº‹: {len(product_research_articles)}ä»¶ã‚’å–å¾—")
            print(f"ğŸ“Š ãƒ•ã‚£ãƒ¼ãƒ‰å…¨ä½“: {len(all_articles)}ä»¶ä¸­ {len(product_research_articles)}ä»¶ã‚’é¸åˆ¥")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å†…è¨³ã‚’è¡¨ç¤º
            category_counts = {}
            for article in all_articles:
                cat = article['category'] or 'ãã®ä»–'
                category_counts[cat] = category_counts.get(cat, 0) + 1
            
            print("ğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥å†…è¨³:")
            for cat, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
                mark = "âœ…" if cat == "Product Research" else "âŒ"
                print(f"  {mark} {cat}: {count}ä»¶")
            
            return product_research_articles
            
        except Exception as e:
            print(f"âŒ RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _is_product_research(self, entry) -> bool:
        """Product Researchã‚«ãƒ†ã‚´ãƒªã®è¨˜äº‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # æ–¹æ³•1: categoryå±æ€§ã§åˆ¤å®šï¼ˆæœ€ã‚‚ç¢ºå®Ÿï¼‰
        if hasattr(entry, 'category') and entry.category == 'Product Research':
            return True
        
        # æ–¹æ³•2: tagsã§åˆ¤å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if hasattr(entry, 'tags'):
            for tag in entry.tags:
                if tag.term == 'Product Research':
                    return True
        
        return False
    
    def _clean_html_content(self, content: str) -> str:
        """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹"""
        if not content:
            return ""
        
        # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        content = html.unescape(content)
        
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        content = re.sub(r'<[^>]+>', '', content)
        
        # ä½™åˆ†ãªç©ºç™½æ–‡å­—ã‚’é™¤å»
        content = re.sub(r'\s+', ' ', content)
        content = content.strip()
        
        return content
    
    def _extract_content(self, entry) -> str:
        """è¨˜äº‹ã®æœ¬æ–‡å†…å®¹ã‚’æŠ½å‡º"""
        content = ""
        if hasattr(entry, 'content'):
            content = entry.content[0].value if entry.content else ''
        elif hasattr(entry, 'summary'):
            content = entry.summary
        
        # HTMLã‚¿ã‚°ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        return self._clean_html_content(content)
    
    def select_top_articles(self, articles: List[Dict], count: int = 5) -> List[Dict]:
        """äººæ°—ãƒ»é–¢é€£æ€§ã®é«˜ã„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆè¨˜äº‹ã‚’é¸åˆ¥"""
        print(f"ğŸ” ä¸Šä½{count}ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆè¨˜äº‹ã‚’é¸åˆ¥ä¸­...")
        print("âš¡ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–: è©±é¡Œæ€§ãƒ»ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆé­…åŠ›åº¦ãƒ»æ–°ã—ã•ãƒ»è¨˜äº‹å……å®Ÿåº¦")
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ï¼ˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒå°‚ç”¨ï¼‰
        def score_article(article: Dict) -> float:
            score = 0
            
            # ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®é­…åŠ›åº¦ï¼ˆç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒœãƒ¼ãƒŠã‚¹ï¼‰
            high_impact_keywords = [
                'AI', 'è‡ªå‹•åŒ–', 'åŠ¹ç‡åŒ–', 'é©æ–°çš„', 'ç”»æœŸçš„', 'æ¬¡ä¸–ä»£',
                'ãƒªãƒªãƒ¼ã‚¹', 'ç™ºè¡¨', 'æœ€æ–°', 'ç„¡æ–™', 'ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹',
                'ãƒãƒ¼ã‚³ãƒ¼ãƒ‰', 'ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰', 'ãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹', 'API'
            ]
            
            for keyword in high_impact_keywords:
                if keyword in article['title']:
                    score += 2
            
            # ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆé–¢é€£ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            product_priority_keywords = [
                'ãƒ„ãƒ¼ãƒ«', 'ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ã‚¢ãƒ—ãƒª', 
                'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 'SaaS', 'ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰', 'ã‚¨ãƒ‡ã‚£ã‚¿',
                'ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼', 'ãƒ“ãƒ«ãƒ€ãƒ¼', 'ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼'
            ]
            
            for keyword in product_priority_keywords:
                if keyword in article['title'] or keyword in article['summary']:
                    score += 1.5
            
            # æŠ€è¡“åˆ†é‡ã®å¤šæ§˜æ€§ãƒœãƒ¼ãƒŠã‚¹
            tech_categories = [
                'AI', 'Webé–‹ç™º', 'ãƒ¢ãƒã‚¤ãƒ«', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'ãƒ‡ãƒ¼ã‚¿åˆ†æ', 'æ¥­å‹™åŠ¹ç‡åŒ–', 'ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³'
            ]
            
            for category in tech_categories:
                if category in article['title'] or category in ' '.join(article['tags']):
                    score += 1
            
            # è¨˜äº‹ã®æ–°ã—ã•ï¼ˆæœ€è¿‘ã®è¨˜äº‹ã‚’å„ªé‡ï¼‰
            try:
                if article['published']:
                    pub_date = datetime.strptime(article['published'][:19], '%Y-%m-%dT%H:%M:%S')
                    days_ago = (datetime.now() - pub_date).days
                    if days_ago <= 1:
                        score += 5  # ä»Šæ—¥ãƒ»æ˜¨æ—¥ã®è¨˜äº‹ã¯é«˜å¾—ç‚¹
                    elif days_ago <= 7:
                        score += 3
                    elif days_ago <= 30:
                        score += 1
            except:
                pass
            
            # è¦ç´„ã®å……å®Ÿåº¦
            if len(article['summary']) > 100:
                score += 1
            
            return score
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        scored_articles = [(article, score_article(article)) for article in articles]
        scored_articles.sort(key=lambda x: x[1], reverse=True)
        
        selected = [article for article, score in scored_articles[:count]]
        
        print(f"âœ… é¸åˆ¥å®Œäº†:")
        for i, (article, score) in enumerate(scored_articles[:count], 1):
            print(f"  {i}. {article['title'][:60]}... (ã‚¹ã‚³ã‚¢: {score:.1f})")
        
        return selected
    
    def _extract_keyword_from_articles(self, selected_articles: List[Dict]) -> str:
        """è¨˜äº‹ã‹ã‚‰ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒæ¤œç´¢ã«é©ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒæ¤œç´¢ã«é©ã—ãŸä¸€èˆ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œ
        keyword_candidates = []
        
        # æŠ€è¡“ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        tech_keywords = ['AI', 'è‡ªå‹•åŒ–', 'éŸ³å£°', 'ç”»åƒ', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'é–‹ç™º', 'ã‚³ãƒ¼ãƒ‰', 'IT', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢']
        # ä¸€èˆ¬ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  
        general_keywords = ['åŠ¹ç‡', 'ä¾¿åˆ©', 'ç„¡æ–™', 'ç°¡å˜', 'é©æ–°', 'æœªæ¥', 'ä½œæ¥­åŠ¹ç‡åŒ–', 'ã‚¢ãƒ—ãƒª', 'ã‚²ãƒ¼ãƒ ']
        # æ„Ÿæƒ…ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        emotion_keywords = ['é©šã', 'æ¥½ã—ã„', 'å®‰å¿ƒ', 'ä¾¿åˆ©', 'å¿«é©']
        
        all_text = ""
        for article in selected_articles:
            all_text += f"{article['title']} {article['summary']} {' '.join(article['tags'])}"
        
        all_text = all_text.lower()
        
        # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ãƒãƒƒãƒã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†
        for keyword in tech_keywords + general_keywords + emotion_keywords:
            if keyword.lower() in all_text:
                keyword_candidates.append(keyword)
        
        # ãƒãƒƒãƒã—ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not keyword_candidates:
            keyword_candidates = ['AI', 'ãƒ„ãƒ¼ãƒ«', 'ä¾¿åˆ©', 'åŠ¹ç‡', 'æœªæ¥']
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
        import random
        return random.choice(keyword_candidates)
    
    def _generate_article_title(self, selected_articles: List[Dict], keyword: str) -> str:
        """é­…åŠ›çš„ã§å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ"""
        import random
        
        # è¨˜äº‹ã®ç‰¹å¾´ã‚’åˆ†æ
        has_ai = any('AI' in article['title'] or 'ai' in article['title'].lower() for article in selected_articles)
        has_automation = any('è‡ªå‹•' in article['title'] or 'automation' in article['title'].lower() for article in selected_articles)
        has_design = any('ãƒ‡ã‚¶ã‚¤ãƒ³' in article['title'] or 'design' in article['title'].lower() for article in selected_articles)
        has_dev_tools = any('é–‹ç™º' in article['title'] or 'é–‹ç™ºè€…' in ' '.join(article['tags']) for article in selected_articles)
        
        # ç‰¹å¾´ã«å¿œã˜ãŸã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
        if has_ai:
            title_patterns = [
                f"AIã§ä½œæ¥­åŠ¹ç‡çˆ†ä¸ŠãŒã‚Šãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"è©±é¡Œã®AIãƒ„ãƒ¼ãƒ«{len(selected_articles)}é¸ã§ç”Ÿç”£æ€§ã‚¢ãƒƒãƒ—",
                f"ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ³¨ç›®ã®AIãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"æœ€æ–°AIæŠ€è¡“ã‚’æ´»ç”¨ã—ãŸãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸"
            ]
        elif has_automation:
            title_patterns = [
                f"è‡ªå‹•åŒ–ã§æ¥½ã«ãªã‚‹ä¾¿åˆ©ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"ä½œæ¥­åŠ¹ç‡åŒ–ã®æ•‘ä¸–ä¸»ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"æ™‚çŸ­ãƒ»è‡ªå‹•åŒ–ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"é¢å€’ãªä½œæ¥­ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸"
            ]
        elif has_design:
            title_patterns = [
                f"ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼å¿…è¦‹ã®æœ€æ–°ãƒ„ãƒ¼ãƒ«{len(selected_articles)}é¸",
                f"ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼å‘ã‘ä¾¿åˆ©ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"ãƒ‡ã‚¶ã‚¤ãƒ³ä½œæ¥­ãŒæ—ã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"ç¾ã—ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ä½œã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸"
            ]
        elif has_dev_tools:
            title_patterns = [
                f"é–‹ç™ºè€…ã®ç”Ÿç”£æ€§ã‚’ä¸Šã’ã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å¿…è¦‹ã®é–‹ç™ºãƒ„ãƒ¼ãƒ«{len(selected_articles)}é¸",
                f"ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæ¥½ã«ãªã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"é–‹ç™ºåŠ¹ç‡åŒ–ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸"
            ]
        else:
            # æ±ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
            title_patterns = [
                f"ä»Šé€±ãƒã‚ºã£ã¦ã‚‹æ³¨ç›®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒé¸ã¶ä¾¿åˆ©ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"è©±é¡Œæ²¸é¨°ä¸­ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"ä½¿ã£ã¦ã¿ãŸããªã‚‹ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸",
                f"é©æ–°çš„ãªãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸ã‚’ã”ç´¹ä»‹",
                f"æ³¨ç›®åº¦æ€¥ä¸Šæ˜‡ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ{len(selected_articles)}é¸"
            ]
        
        main_title = random.choice(title_patterns)
        return f"5åˆ†ã§èª­ã‚ã‚‹ã€{main_title} ã€ä»Šæ—¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šã€Œ{keyword}ã€ã€‘"
    
    def _generate_keyword_blockquote(self, keyword: str, selected_articles: List[Dict]) -> str:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ³ã‚’ç”Ÿæˆ"""
        import random
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥ã®æ„Ÿæƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³
        keyword_comments = {
            'AI': [
                "æœ€è¿‘ã®AIæŠ€è¡“ã®é€²æ­©ã£ã¦æœ¬å½“ã«ç›®ã‚’è¦‹å¼µã‚‹ã‚‚ã®ãŒã‚ã‚Šã¾ã™ã‚ˆã­ã€‚ã“ã‚“ãªã«ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹æ™‚ä»£ã«ç”Ÿãã¦ã„ã‚‹ã“ã¨ã«æ„Ÿè¬ã§ã™ï¼",
                "AIåˆ†é‡ã®é©æ–°ã‚¹ãƒ”ãƒ¼ãƒ‰ãŒã‚‚ã†æ­¢ã¾ã‚‰ãªã„ã§ã™ã­ã€‚æ¯æ—¥æ–°ã—ã„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãŒå‡ºã¦ãã¦ã€æœªæ¥ãŒã©ã‚“ã©ã‚“ç¾å®Ÿã«ãªã£ã¦ã„ãæ„Ÿã˜ãŒãŸã¾ã‚Šã¾ã›ã‚“ï¼",
                "AIãŒã“ã“ã¾ã§èº«è¿‘ã«ãªã‚‹ãªã‚“ã¦ã€æ•°å¹´å‰ã¯æƒ³åƒã‚‚ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æŠ€è¡“ã®åŠ›ã§ä¸–ç•ŒãŒã‚ˆã‚Šè‰¯ããªã£ã¦ã„ãç¬é–“ã‚’ç›®ã®å½“ãŸã‚Šã«ã—ã¦ã„ã‚‹æ°—åˆ†ã§ã™ï¼"
            ],
            'è‡ªå‹•åŒ–': [
                "è‡ªå‹•åŒ–æŠ€è¡“ã®é€²æ­©ã§ã€ã¿ã‚“ãªãŒã‚‚ã£ã¨ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªä»•äº‹ã«é›†ä¸­ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ãã¾ã—ãŸã­ã€‚ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®åŠ›ã£ã¦ã™ã”ã„ï¼",
                "é¢å€’ãªä½œæ¥­ã‚’è‡ªå‹•åŒ–ã—ã¦ãã‚Œã‚‹ãƒ„ãƒ¼ãƒ«ãŒã©ã‚“ã©ã‚“å‡ºã¦ãã¦ã€æœ¬å½“ã«åŠ©ã‹ã‚Šã¾ã™ã€‚ç”Ÿç”£æ€§å‘ä¸Šã«ç›´çµã™ã‚‹ç´ æ™´ã‚‰ã—ã„æ™‚ä»£ã§ã™ã­ï¼",
                "è‡ªå‹•åŒ–ã«ã‚ˆã£ã¦äººé–“ã¯ã‚ˆã‚Šäººé–“ã‚‰ã—ã„ä»•äº‹ã«å°‚å¿µã§ãã‚‹ã€‚ã“ã‚Œã“ããƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãŒç›®æŒ‡ã™ã¹ãæ–¹å‘ã ã¨æ€ã„ã¾ã™ï¼"
            ],
            'éŸ³å£°': [
                "éŸ³å£°æŠ€è¡“ã®é€²æ­©ãŒæœ¬å½“ã«ã™ã”ãã¦ã€ã‚‚ã†äººé–“ã®å£°ã¨åŒºåˆ¥ãŒã¤ã‹ãªã„ãƒ¬ãƒ™ãƒ«ã«ãªã£ã¦ãã¾ã—ãŸã­ã€‚æŠ€è¡“ã®é€²æ­©ã«ãƒ¯ã‚¯ãƒ¯ã‚¯ãŒæ­¢ã¾ã‚Šã¾ã›ã‚“ï¼",
                "éŸ³å£°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒã“ã‚Œã»ã©è‡ªç„¶ã«ãªã‚‹ã¨ã¯ï¼æœªæ¥ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å½¢æ…‹ãŒç¾å®Ÿã«ãªã£ã¦ã„ã‚‹æ„Ÿã˜ãŒã—ã¾ã™ã€‚",
                "éŸ³å£°åˆæˆæŠ€è¡“ã®ç™ºé”ã§ã€æ–°ã—ã„è¡¨ç¾ã®å¯èƒ½æ€§ãŒç„¡é™ã«åºƒãŒã£ã¦ã„ã¾ã™ã­ã€‚ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªé ˜åŸŸã§ã®æ´»ç”¨ãŒæ¥½ã—ã¿ã§ã™ï¼"
            ],
            'ãƒ‡ã‚¶ã‚¤ãƒ³': [
                "ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ã®é€²åŒ–ãŒæ­¢ã¾ã‚‰ãªã„ã§ã™ã­ï¼èª°ã§ã‚‚ãƒ—ãƒ­ç´šã®ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã§ãã‚‹æ™‚ä»£ã«ãªã£ã¦ã€å¯èƒ½æ€§ãŒç„¡é™å¤§ã«æ„Ÿã˜ã¾ã™ã€‚",
                "ç¾ã—ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’ç°¡å˜ã«ä½œã‚Œã‚‹ãƒ„ãƒ¼ãƒ«ãŒã©ã‚“ã©ã‚“ç™»å ´ã—ã¦ã€å‰µä½œæ´»å‹•ã®ãƒãƒ¼ãƒ‰ãƒ«ãŒã‚°ãƒƒã¨ä¸‹ãŒã‚Šã¾ã—ãŸã­ã€‚ç´ æ™´ã‚‰ã—ã„æ™‚ä»£ã§ã™ï¼",
                "ãƒ‡ã‚¶ã‚¤ãƒ³ã¨ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®èåˆãŒç”Ÿã¿å‡ºã™ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã«ã€ã„ã¤ã‚‚ãƒ¯ã‚¯ãƒ¯ã‚¯ã•ã›ã‚‰ã‚Œã¾ã™ã€‚æœªæ¥ã¯ã‚‚ã£ã¨ç¾ã—ããªã‚Šãã†ï¼"
            ],
            'é–‹ç™º': [
                "é–‹ç™ºãƒ„ãƒ¼ãƒ«ã®é€²æ­©ã®ãŠã‹ã’ã§ã€ä½œæ¥­ã®ç”Ÿç”£æ€§ãŒã©ã‚“ã©ã‚“å‘ä¸Šã—ã¦ã„ã¾ã™ã­ã€‚æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å½¢ã«ã™ã‚‹ã®ãŒã‚ˆã‚Šæ¥½ã—ããªã£ã¦ã„ã¾ã™ï¼",
                "é–‹ç™ºç’°å¢ƒãŒã“ã‚Œã»ã©ä¾¿åˆ©ã«ãªã‚‹ãªã‚“ã¦æœ¬å½“ã«ã™ã”ã„ã§ã™ã€‚ç´ æ™´ã‚‰ã—ã„ãƒ„ãƒ¼ãƒ«ã‚’ä½œã£ã¦ãã‚Œã‚‹é–‹ç™ºè€…ã®çš†ã•ã‚“ã«æ„Ÿè¬ã§ã™ï¼",
                "æ–°ã—ã„é–‹ç™ºãƒ„ãƒ¼ãƒ«ãŒå‡ºã‚‹ãŸã³ã«ã€ã€Œã‚‚ã£ã¨æ—©ãå‡ºä¼šã„ãŸã‹ã£ãŸï¼ã€ã¨æ€ã£ã¦ã—ã¾ã„ã¾ã™ã€‚æŠ€è¡“ã®é€²æ­©ã£ã¦æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„ï¼"
            ],
            'ãƒ„ãƒ¼ãƒ«': [
                "ä¾¿åˆ©ãªãƒ„ãƒ¼ãƒ«ãŒã“ã‚Œã ã‘ãŸãã•ã‚“å‡ºã¦ãã‚‹ã¨ã€é¸ã¶ã®ã‚‚æ¥½ã—ã„æ‚©ã¿ã§ã™ã­ã€‚ã“ã‚“ãªã«æµã¾ã‚ŒãŸç’°å¢ƒã«ã„ã‚‹ã“ã¨ã«æ„Ÿè¬ã§ã™ï¼",
                "æ–°ã—ã„ãƒ„ãƒ¼ãƒ«ã¨ã®å‡ºä¼šã„ã£ã¦ã„ã¤ã‚‚ãƒ¯ã‚¯ãƒ¯ã‚¯ã—ã¾ã™ã‚ˆã­ã€‚ç”Ÿç”£æ€§å‘ä¸Šã ã‘ã§ãªãã€æ–°ã—ã„ç™ºè¦‹ã‚‚ã‚ã£ã¦æœ€é«˜ã§ã™ï¼",
                "é©æ–°çš„ãªãƒ„ãƒ¼ãƒ«ãŒæ¬¡ã€…ã¨ç™»å ´ã™ã‚‹ç¾ä»£ã£ã¦ã€æœ¬å½“ã«å¹¸ã›ãªæ™‚ä»£ã ã¨æ€ã„ã¾ã™ã€‚å¯èƒ½æ€§ãŒç„¡é™å¤§ã§ã™ã­ï¼"
            ]
        }
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚³ãƒ¡ãƒ³ãƒˆ
        default_comments = [
            "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®é€²æ­©ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã«æ¯æ—¥é©šã‹ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚“ãªã«ã‚¨ã‚­ã‚µã‚¤ãƒ†ã‚£ãƒ³ã‚°ãªæ™‚ä»£ã«ç”Ÿãã¦ã„ã‚‹ã“ã¨ãŒå¬‰ã—ã„ã§ã™ï¼",
            "æ–°ã—ã„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã¨ã®å‡ºä¼šã„ã¯ã€ã„ã¤ã‚‚æ–°ã—ã„å¯èƒ½æ€§ã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã­ã€‚ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®åŠ›ã£ã¦ã™ã”ã„ï¼",
            "é©æ–°çš„ãªãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãŒæ¬¡ã€…ã¨ç”Ÿã¾ã‚Œã‚‹ç¾ä»£ã£ã¦ã€æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„æ™‚ä»£ã ã¨æ€ã„ã¾ã™ã€‚æœªæ¥ã¸ã®æœŸå¾…ãŒè†¨ã‚‰ã¿ã¾ã™ï¼"
        ]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        comments = keyword_comments.get(keyword, default_comments)
        return random.choice(comments)
    
    def _extract_product_tags(self, selected_articles: List[Dict]) -> List[str]:
        """ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆå›ºæœ‰åã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’æŠ½å‡º"""
        product_tags = []
        
        for article in selected_articles:
            title = article['title']
            
            # ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåã‚’æŠ½å‡ºï¼ˆâ€“ ã¾ãŸã¯ - ã‚ˆã‚Šå‰ã®éƒ¨åˆ†ï¼‰
            product_name = re.split(r'[â€“\-]', title)[0].strip()
            
            # ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»ã—ã¦ã‚¿ã‚°åŒ–
            clean_name = re.sub(r'[^\w\d]', '', product_name)
            if len(clean_name) > 2:  # 3æ–‡å­—ä»¥ä¸Šã®ã‚‚ã®ã®ã¿
                product_tags.append(f"#{clean_name}")
        
        return product_tags

    async def _generate_all_content_with_claude(self, selected_articles: List[Dict]) -> Dict[str, str]:
        """Claude APIã§å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµ±åˆç”Ÿæˆï¼ˆåŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        print("ğŸ¤– Claude APIã§çµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆä¸­...")
        
        # è¨˜äº‹æƒ…å ±ã‚’ã¾ã¨ã‚ã‚‹
        articles_info = []
        for i, article in enumerate(selected_articles, 1):
            articles_info.append(f"""
è¨˜äº‹{i}:
ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}
è¦ç´„: {article['summary'][:200]}...
ã‚¿ã‚°: {', '.join(article['tags'][:5])}
""")
        
        unified_prompt = f"""
ä»¥ä¸‹ã®{len(selected_articles)}ã¤ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆè¨˜äº‹ã‚’åˆ†æã—ã¦ã€Note.comè¨˜äº‹ã«å¿…è¦ãªè¦ç´ ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹æƒ…å ±:
{''.join(articles_info)}

ä»¥ä¸‹ã®4ã¤ã®è¦ç´ ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

1. **ã‚¢ã‚¤ã‚­ãƒ£ãƒƒãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: Note.comã§æ–°ã—ãã¦è³ªã®é«˜ã„ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã‚„ã™ã„1èª
2. **blockquoteã‚³ãƒ¡ãƒ³ãƒˆ**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹ãƒã‚¸ãƒ†ã‚£ãƒ–ã§è¦ªã—ã¿ã‚„ã™ã„æ„Ÿæƒ³ï¼ˆ1-2è¡Œï¼‰
3. **è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«**: ã€Œ5åˆ†ã§èª­ã‚ã‚‹ã€[é­…åŠ›çš„ãªå†…å®¹]{len(selected_articles)}é¸ ã€ä»Šæ—¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šã€Œ[ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰]ã€ã€‘ã€å½¢å¼
4. **ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°**: ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåã‹ã‚‰ç”Ÿæˆã—ãŸé©åˆ‡ãªãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼ˆæœ€å¤§5å€‹ï¼‰

å‡ºåŠ›å½¢å¼ï¼ˆå¿…é ˆï¼‰:
```
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: [1èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰]
blockquote: [è¦ªã—ã¿ã‚„ã™ã„ã‚³ãƒ¡ãƒ³ãƒˆæ–‡]
ã‚¿ã‚¤ãƒˆãƒ«: [å®Œæ•´ãªã‚¿ã‚¤ãƒˆãƒ«]
ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: #tag1 #tag2 #tag3 #tag4 #tag5
```

æ³¨æ„äº‹é …:
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯Note.comã§é »ç¹ã«æ›´æ–°ã•ã‚Œã‚‹åˆ†é‡ã‹ã‚‰é¸æŠ
- blockquoteã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ç”¨èªã‚’é¿ã‘ã¦ä¸€èˆ¬èª­è€…å‘ã‘ã«
- ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¯è‹±æ•°å­—ã®ã¿ã€3æ–‡å­—ä»¥ä¸Š
- ã™ã¹ã¦ä»Šå›ã®è¨˜äº‹å†…å®¹ã«é–¢é€£ã•ã›ã‚‹
"""

        try:
            response = await self._call_claude_api_for_content(unified_prompt, max_tokens=800)
            
            if response:
                parsed_content = self._parse_unified_response(response, selected_articles)
                if parsed_content:
                    print("âœ… Claudeçµ±åˆç”Ÿæˆå®Œäº†")
                    return parsed_content
            
            print("âš ï¸ Claude APIçµ±åˆç”Ÿæˆå¤±æ•—ã€å€‹åˆ¥ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return await self._generate_individual_content(selected_articles)
            
        except Exception as e:
            print(f"âŒ Claude APIçµ±åˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return await self._generate_individual_content(selected_articles)

    def _parse_unified_response(self, response: str, selected_articles: List[Dict]) -> Dict[str, str]:
        """çµ±åˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
        try:
            result = {}
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keyword_match = re.search(r'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰[:\s]*([^\n\r]+)', response)
            if keyword_match:
                result['keyword'] = keyword_match.group(1).strip()
            
            # blockquoteæŠ½å‡º
            blockquote_match = re.search(r'blockquote[:\s]*([^\n\r]+(?:\n[^#\n]*)*)', response, re.MULTILINE)
            if blockquote_match:
                result['blockquote'] = blockquote_match.group(1).strip()
            
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title_match = re.search(r'ã‚¿ã‚¤ãƒˆãƒ«[:\s]*([^\n\r]+)', response)
            if title_match:
                result['title'] = title_match.group(1).strip()
            
            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ½å‡º
            hashtag_match = re.search(r'ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°[:\s]*([^\n\r]+)', response)
            if hashtag_match:
                hashtags_line = hashtag_match.group(1).strip()
                hashtags = re.findall(r'#\w+', hashtags_line)
                result['hashtags'] = hashtags
            
            # å¿…é ˆè¦ç´ ãŒæƒã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            required_keys = ['keyword', 'blockquote', 'title']
            if all(key in result for key in required_keys):
                return result
            else:
                print(f"âš ï¸ å¿…é ˆè¦ç´ ä¸è¶³: {[k for k in required_keys if k not in result]}")
                return None
                
        except Exception as e:
            print(f"âš ï¸ çµ±åˆãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None

    async def _generate_individual_content(self, selected_articles: List[Dict]) -> Dict[str, str]:
        """å€‹åˆ¥ç”Ÿæˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ å€‹åˆ¥ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
        
        # æ”¹è‰¯ç‰ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’ä½¿ç”¨
        keyword = await self._extract_keyword_from_articles_note_optimized(selected_articles)
        blockquote = self._generate_keyword_blockquote(keyword, selected_articles)
        hashtags = self._extract_product_tags(selected_articles)
        title = self._generate_article_title(selected_articles, keyword)
        
        return {
            'keyword': keyword,
            'blockquote': blockquote, 
            'title': title,
            'hashtags': hashtags
        }

    async def _extract_keyword_from_articles_note_optimized(self, selected_articles: List[Dict]) -> str:
        """Note.comæœ€é©åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        print("ğŸ¯ Note.comæœ€é©åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­...")
        
        # è¨˜äº‹å†…å®¹ã‚’åˆ†æ
        all_text = ""
        for article in selected_articles:
            all_text += f"{article['title']} {article['summary']} {' '.join(article['tags'])}"
        
        all_text_lower = all_text.lower()
        
        # Note.comæœ€é©åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç¢ºå®Ÿã«è‰¯ã„ç”»åƒãŒã‚ã‚‹ã‚‚ã®ï¼‰
        priority_keywords = {
            'AI': ['ai', 'artificial intelligence', 'äººå·¥çŸ¥èƒ½'],
            'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—': ['startup', 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'èµ·æ¥­'],
            'ãƒ“ã‚¸ãƒã‚¹': ['business', 'ãƒ“ã‚¸ãƒã‚¹', 'äº‹æ¥­'],
            'ãƒ‡ã‚¶ã‚¤ãƒ³': ['design', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'ui', 'ux'],
            'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼': ['technology', 'tech', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'æŠ€è¡“'],
            'ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³': ['innovation', 'ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³', 'é©æ–°'],
            'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ': ['product', 'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ', 'ãƒ„ãƒ¼ãƒ«', 'tool'],
            'ãƒ¯ãƒ¼ã‚¯ã‚¹ã‚¿ã‚¤ãƒ«': ['work', 'ãƒªãƒ¢ãƒ¼ãƒˆ', 'remote', 'åƒãæ–¹'],
            'DX': ['dx', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'digital', 'å¤‰é©'],
            'ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–': ['creative', 'ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–', 'ã‚¢ãƒ¼ãƒˆ']
        }
        
        # ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢è¨ˆç®—
        keyword_scores = {}
        for keyword, patterns in priority_keywords.items():
            score = 0
            for pattern in patterns:
                score += all_text_lower.count(pattern.lower())
            if score > 0:
                keyword_scores[keyword] = score
        
        # ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆç‰¹åŒ–ã®å ´åˆã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«
        product_indicators = ['ãƒ„ãƒ¼ãƒ«', 'ã‚¢ãƒ—ãƒª', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢']
        has_product_focus = any(indicator in all_text for indicator in product_indicators)
        
        if has_product_focus:
            product_specific_keywords = ['ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ', 'ãƒ„ãƒ¼ãƒ«', 'ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢']
            for keyword in product_specific_keywords:
                if keyword in all_text:
                    keyword_scores[keyword] = keyword_scores.get(keyword, 0) + 3  # ãƒœãƒ¼ãƒŠã‚¹
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ
        if keyword_scores:
            best_keyword = max(keyword_scores, key=keyword_scores.get)
            print(f"ğŸ¯ Noteæœ€é©åŒ–é¸æŠ: ã€Œ{best_keyword}ã€(ã‚¹ã‚³ã‚¢: {keyword_scores[best_keyword]})")
            return best_keyword
        
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        safe_keywords = ['ãƒ“ã‚¸ãƒã‚¹', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³', 'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ', 'ãƒ‡ã‚¶ã‚¤ãƒ³']
        import random
        selected = random.choice(safe_keywords)
        print(f"ğŸ¯ å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã€Œ{selected}ã€")
        return selected
    
    async def generate_article_with_claude(self, selected_articles: List[Dict]) -> str:
        """Claude APIã‚’ä½¿ã£ã¦è¨˜äº‹ã‚’ç”Ÿæˆï¼ˆçµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆå¯¾å¿œï¼‰"""
        print("ğŸ¤– Claude APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # çµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚’è©¦è¡Œ
        content_elements = await self._generate_all_content_with_claude(selected_articles)
        
        if not content_elements:
            print("âš ï¸ çµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆå¤±æ•—ã€å¾“æ¥æ–¹å¼ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return await self._generate_article_traditional(selected_articles)
        
        # çµ±åˆç”Ÿæˆã•ã‚ŒãŸè¦ç´ ã‚’ä½¿ç”¨
        keyword = content_elements['keyword']
        blockquote = content_elements['blockquote']
        title = content_elements['title']
        hashtags = content_elements.get('hashtags', [])
        
        print(f"ğŸ¯ çµ±åˆç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œ{keyword}ã€")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        articles_info = []
        for i, article in enumerate(selected_articles, 1):
            articles_info.append(f"""
è¨˜äº‹{i}:
ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}
URL: {article['link']}
è¦ç´„: {article['summary'][:200]}...
ã‚¿ã‚°: {', '.join(article['tags'][:5])}
""")
        
        prompt = f"""
ä»¥ä¸‹ã®Peaky Mediaã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒè¨˜äº‹ã‚’å‚è€ƒã«ã€Note.comå‘ã‘ã®è¦ªã—ã¿ã‚„ã™ã„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã¾ã¨ã‚è¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

å‚è€ƒè¨˜äº‹ï¼ˆå…¨ã¦Product Researchã‚«ãƒ†ã‚´ãƒªï¼‰:
{''.join(articles_info)}

ä½œæˆã™ã‚‹è¨˜äº‹ã®è¦ä»¶:
1. ã‚¿ã‚¤ãƒˆãƒ«: ã€Œ{title}ã€ï¼ˆæ—¢ã«æ±ºå®šæ¸ˆã¿ï¼‰
2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰blockquote: ã€Œ{blockquote}ã€ï¼ˆæ—¢ã«æ±ºå®šæ¸ˆã¿ï¼‰
3. å°å…¥éƒ¨: æ°—è»½ã§è¦ªã—ã¿ã‚„ã™ã„å°å…¥æ–‡ï¼ˆ100-150æ–‡å­—ç¨‹åº¦ï¼‰
4. å„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆç´¹ä»‹: {len(selected_articles)}ã¤ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’200-350æ–‡å­—ã§è¦ªã—ã¿ã‚„ã™ãç´¹ä»‹
5. Peaky Mediaãƒªãƒ³ã‚¯: ã€Œå…¬å¼ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã€æ¯æ—¥ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ï¼ãŠæ°—è»½ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„â¬‡ï¸ã€ã§URLå˜ä½“é…ç½®
6. è¦ªã—ã¿ã‚„ã™ã„çµã³: èª­è€…ã¨ã®è·é›¢æ„Ÿã‚’ç¸®ã‚ã‚‹å‘¼ã³ã‹ã‘
7. ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: {', '.join(hashtags)} + ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆé–¢é€£ã‚¿ã‚°

æ–‡ä½“ã®ç‰¹å¾´:
- è¦ªã—ã¿ã‚„ã™ãè©±ã—ã‹ã‘ã‚‹ã‚ˆã†ãªæ–‡ä½“
- ã€Œã“ã‚Œã¯ä¾¿åˆ©ãã†ï¼ã€ã€Œè©¦ã—ã¦ã¿ãŸããªã‚‹ã€ã¨ã„ã†è¡¨ç¾
- å°‚é–€ç”¨èªã‚’é¿ã‘ã€ä¸€èˆ¬ã®æ–¹ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã
- Noteèª­è€…å±¤ã«åˆã‚ã›ãŸæ¸©ã‹ã¿ã®ã‚ã‚‹è¡¨ç¾
- ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®é­…åŠ›ã‚’åˆ†ã‹ã‚Šã‚„ã™ãä¼ãˆã‚‹

ãƒªãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿æ–¹ï¼ˆé‡è¦ï¼‰:
- è©³ç´°è¨˜äº‹: URLå˜ä½“ã§é…ç½®ï¼ˆNote.comãŒè‡ªå‹•ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚«ãƒ¼ãƒ‰ã‚’è¡¨ç¤ºï¼‰
- Peaky Media: ã€Œhttps://peaky.co.jp/ã€ï¼ˆURLå˜ä½“ã§é…ç½®ï¼‰
â€»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒªãƒ³ã‚¯ã¯ä½¿ã‚ãšã€URLå˜ä½“ã§é…ç½®ã—ã¦ãã ã•ã„
â€»ã€Œè©³ã—ãã¯ã“ã¡ã‚‰ï¼šã€ãªã©ã®ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã¯ä¸è¦ã§ã™

è¨˜äº‹æ§‹æˆ:
# {title}

> {blockquote}

è¦ªã—ã¿ã‚„ã™ã„å°å…¥æ–‡

## 1. ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆå
ç´¹ä»‹æ–‡

URL

...ï¼ˆ{len(selected_articles)}ã¤ç¹°ã‚Šè¿”ã—ï¼‰

## æœ€å¾Œã«
è¦ªã—ã¿ã‚„ã™ã„çµã³ + ã€Œå…¬å¼ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã€æ¯æ—¥ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ï¼ãŠæ°—è»½ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„â¬‡ï¸ã€ + Peaky Mediaã®URLå˜ä½“é…ç½®
ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°

Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

        try:
            # Claude APIã«é€ä¿¡
            response = await self._call_claude_api(prompt)
            
            if response:
                print("âœ… Claude APIã§è¨˜äº‹ç”Ÿæˆå®Œäº†")
                return response
            else:
                return self._generate_fallback_article(selected_articles)
                
        except Exception as e:
            print(f"âŒ Claude APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_fallback_article(selected_articles)

    async def _generate_article_traditional(self, selected_articles: List[Dict]) -> str:
        """å¾“æ¥æ–¹å¼ã§ã®è¨˜äº‹ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        print("ğŸ”„ å¾“æ¥æ–¹å¼ã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        keyword = self._extract_keyword_from_articles(selected_articles)
        print(f"ğŸ¯ æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œ{keyword}ã€")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        articles_info = []
        for i, article in enumerate(selected_articles, 1):
            articles_info.append(f"""
è¨˜äº‹{i}:
ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}
URL: {article['link']}
è¦ç´„: {article['summary'][:200]}...
ã‚¿ã‚°: {', '.join(article['tags'][:5])}
""")
        
        prompt = f"""
ä»¥ä¸‹ã®Peaky Mediaã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒè¨˜äº‹ã‚’å‚è€ƒã«ã€Note.comå‘ã‘ã®è¦ªã—ã¿ã‚„ã™ã„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã¾ã¨ã‚è¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

å‚è€ƒè¨˜äº‹ï¼ˆå…¨ã¦Product Researchã‚«ãƒ†ã‚´ãƒªï¼‰:
{''.join(articles_info)}

ä½œæˆã™ã‚‹è¨˜äº‹ã®è¦ä»¶:
1. ã‚¿ã‚¤ãƒˆãƒ«: ã€Œ5åˆ†ã§èª­ã‚ã‚‹ã€[å…·ä½“çš„ã§é­…åŠ›çš„ãªå†…å®¹] ã€ä»Šæ—¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šã€Œ{keyword}ã€ã€‘ã€ã®å½¢å¼
2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰blockquote: è¨˜äº‹å†’é ­ã«ã€Œ{keyword}ã€ã«ã¤ã„ã¦ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã‚„ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãŒå¥½ããªãƒã‚¸ãƒ†ã‚£ãƒ–ãªäººã®è¦–ç‚¹ã‹ã‚‰ã®1-2è¡Œã‚³ãƒ¡ãƒ³ãƒˆ
3. å°å…¥éƒ¨: æ°—è»½ã§è¦ªã—ã¿ã‚„ã™ã„å°å…¥æ–‡ï¼ˆ100-150æ–‡å­—ç¨‹åº¦ï¼‰
4. å„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆç´¹ä»‹: {len(selected_articles)}ã¤ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’200-350æ–‡å­—ã§è¦ªã—ã¿ã‚„ã™ãç´¹ä»‹
5. Peaky Mediaãƒªãƒ³ã‚¯: ã€Œå…¬å¼ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã€æ¯æ—¥ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ï¼ãŠæ°—è»½ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„â¬‡ï¸ã€ã§URLå˜ä½“é…ç½®
6. è¦ªã—ã¿ã‚„ã™ã„çµã³: èª­è€…ã¨ã®è·é›¢æ„Ÿã‚’ç¸®ã‚ã‚‹å‘¼ã³ã‹ã‘
7. ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°: ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆå + ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆé–¢é€£ã‚¿ã‚°

æ–‡ä½“ã®ç‰¹å¾´:
- è¦ªã—ã¿ã‚„ã™ãè©±ã—ã‹ã‘ã‚‹ã‚ˆã†ãªæ–‡ä½“
- ã€Œã“ã‚Œã¯ä¾¿åˆ©ãã†ï¼ã€ã€Œè©¦ã—ã¦ã¿ãŸããªã‚‹ã€ã¨ã„ã†è¡¨ç¾
- å°‚é–€ç”¨èªã‚’é¿ã‘ã€ä¸€èˆ¬ã®æ–¹ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã
- Noteèª­è€…å±¤ã«åˆã‚ã›ãŸæ¸©ã‹ã¿ã®ã‚ã‚‹è¡¨ç¾
- ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®é­…åŠ›ã‚’åˆ†ã‹ã‚Šã‚„ã™ãä¼ãˆã‚‹

ãƒªãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿æ–¹ï¼ˆé‡è¦ï¼‰:
- è©³ç´°è¨˜äº‹: URLå˜ä½“ã§é…ç½®ï¼ˆNote.comãŒè‡ªå‹•ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚«ãƒ¼ãƒ‰ã‚’è¡¨ç¤ºï¼‰
- Peaky Media: ã€Œhttps://peaky.co.jp/ã€ï¼ˆURLå˜ä½“ã§é…ç½®ï¼‰
â€»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒªãƒ³ã‚¯ã¯ä½¿ã‚ãšã€URLå˜ä½“ã§é…ç½®ã—ã¦ãã ã•ã„
â€»ã€Œè©³ã—ãã¯ã“ã¡ã‚‰ï¼šã€ãªã©ã®ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã¯ä¸è¦ã§ã™

è¨˜äº‹æ§‹æˆ:
# 5åˆ†ã§èª­ã‚ã‚‹ã€[å…·ä½“çš„ã§é­…åŠ›çš„ãªã‚¿ã‚¤ãƒˆãƒ«] ã€ä»Šæ—¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šã€Œ{keyword}ã€ã€‘

> ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«ã¤ã„ã¦ã®ãƒã‚¸ãƒ†ã‚£ãƒ–ãªã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ1-2è¡Œã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è¡¨ç¾ã¯é¿ã‘ã‚‹ï¼‰

è¦ªã—ã¿ã‚„ã™ã„å°å…¥æ–‡

## 1. ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆå
ç´¹ä»‹æ–‡

URL

...ï¼ˆ{len(selected_articles)}ã¤ç¹°ã‚Šè¿”ã—ï¼‰

## æœ€å¾Œã«
è¦ªã—ã¿ã‚„ã™ã„çµã³ + ã€Œå…¬å¼ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã€æ¯æ—¥ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ï¼ãŠæ°—è»½ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„â¬‡ï¸ã€ + Peaky Mediaã®URLå˜ä½“é…ç½®
ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°

Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

        try:
            # Claude APIã«é€ä¿¡
            response = await self._call_claude_api(prompt)
            
            if response:
                print("âœ… å¾“æ¥æ–¹å¼ã§è¨˜äº‹ç”Ÿæˆå®Œäº†")
                return response
            else:
                return self._generate_fallback_article(selected_articles)
                
        except Exception as e:
            print(f"âŒ å¾“æ¥æ–¹å¼APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_fallback_article(selected_articles)
    
    async def _call_claude_api(self, prompt: str) -> str:
        """Claude APIã‚’å‘¼ã³å‡ºã—"""
        url = "https://api.anthropic.com/v1/messages"
        
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.anthropic_api_key,
            "anthropic-version": "2023-06-01"
        }
        
        data = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": 2000,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 401:
                print("âŒ èªè¨¼ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™")
                return None
            elif response.status_code == 404:
                print("âŒ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            response.raise_for_status()
            result = response.json()
            return result['content'][0]['text']
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIé€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        except (KeyError, IndexError) as e:
            print(f"âŒ APIãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None

    async def _call_claude_api_for_content(self, prompt: str, max_tokens: int = 800) -> str:
        """çµ±åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆç”¨ã®Claude APIå‘¼ã³å‡ºã—"""
        url = "https://api.anthropic.com/v1/messages"
        
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.anthropic_api_key,
            "anthropic-version": "2023-06-01"
        }
        
        data = {
            "model": "claude-3-5-sonnet-20241022",
            "max_tokens": max_tokens,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 401:
                print("âŒ èªè¨¼ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™")
                return None
            elif response.status_code == 404:
                print("âŒ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            response.raise_for_status()
            result = response.json()
            return result['content'][0]['text']
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIé€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        except (KeyError, IndexError) as e:
            print(f"âŒ APIãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _generate_fallback_article(self, selected_articles: List[Dict]) -> str:
        """ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒå°‚ç”¨ã®è¦ªã—ã¿ã‚„ã™ã„è¨˜äº‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        print("ğŸ“ ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆå°‚ç”¨è¨˜äº‹ã‚’ç”Ÿæˆä¸­...")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        keyword = self._extract_keyword_from_articles(selected_articles)
        print(f"ğŸ¯ æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œ{keyword}ã€")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨blockquoteã‚’ç”Ÿæˆ
        article_title = self._generate_article_title(selected_articles, keyword)
        keyword_comment = self._generate_keyword_blockquote(keyword, selected_articles)
        product_tags = self._extract_product_tags(selected_articles)
        
        article_content = f"""# {article_title}

> {keyword_comment}

ã“ã‚“ã«ã¡ã¯ï¼ä»Šå›ã¯æœ€è¿‘è¦‹ã¤ã‘ãŸé¢ç™½ã„ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’{len(selected_articles)}ã¤ã”ç´¹ä»‹ã—ã¾ã™ã€‚

ã©ã‚Œã‚‚ã€Œã“ã‚Œã¯ä¾¿åˆ©ãã†ï¼ã€ã¨æ€ãˆã‚‹ãƒ„ãƒ¼ãƒ«ã°ã‹ã‚Šã§ã€æ—¥ã€…ã®ä½œæ¥­ã‚’åŠ¹ç‡åŒ–ã—ã¦ãã‚Œãã†ã§ã™ã€‚

"""
        
        for i, article in enumerate(selected_articles, 1):
            # ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåã‚’æŠ½å‡ºï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®æœ€åˆã®éƒ¨åˆ†ï¼‰
            product_name = re.split(r'[â€“\-]', article['title'])[0].strip()
            
            # è¦ç´„ã‚’é©åˆ‡ãªé•·ã•ã«èª¿æ•´
            summary = article['summary']
            if len(summary) > 180:
                summary = summary[:180] + "..."
            elif len(summary) < 60:
                summary = "ã¨ã¦ã‚‚é­…åŠ›çš„ãªãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã§ã€ä½¿ã£ã¦ã¿ãŸããªã‚‹æ©Ÿèƒ½ãŒãŸãã•ã‚“ã‚ã‚Šãã†ã§ã™ã€‚"
                
            article_content += f"""## {i}. {product_name}

{summary}

{article['link']}

"""
        
        article_content += f"""## æœ€å¾Œã«

ä»Šå›ã”ç´¹ä»‹ã—ãŸ{len(selected_articles)}ã¤ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã€ã„ã‹ãŒã§ã—ãŸã§ã—ã‚‡ã†ã‹ï¼Ÿ

æ°—ã«ãªã‚‹ã‚‚ã®ãŒã‚ã‚Œã°ã€ãœã²è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã­ã€‚

å…¬å¼ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã€æ¯æ—¥ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ï¼ãŠæ°—è»½ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„â¬‡ï¸

https://peaky.co.jp/

ã‚‚ã—ä»–ã«ã‚‚ã€Œã“ã‚Œã¯é¢ç™½ã„ï¼ã€ã¨ã„ã†ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’ã”å­˜çŸ¥ã§ã—ãŸã‚‰ã€ã‚³ãƒ¡ãƒ³ãƒˆã§æ•™ãˆã¦ã„ãŸã ã‘ã‚‹ã¨å¬‰ã—ã„ã§ã™ã€‚

---

{' '.join(product_tags)} #ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ #æœ€æ–°ãƒ„ãƒ¼ãƒ« #ä¾¿åˆ©ã‚µãƒ¼ãƒ“ã‚¹ #ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ #ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒãƒ³ãƒˆ
"""
        
        return article_content
    
    def save_article(self, content: str) -> str:
        """è¨˜äº‹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ä»˜ããƒ»JSTå¯¾å¿œï¼‰"""
        # JSTæ™‚é–“ã§æ—¥ä»˜ã‚’å–å¾—ï¼ˆGitHub Actions UTCç’°å¢ƒå¯¾å¿œï¼‰
        import pytz
        try:
            jst = pytz.timezone('Asia/Tokyo')
            today = datetime.now(jst).strftime("%Y%m%d")
            timezone_info = "JST"
        except ImportError:
            # pytzãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            # ç’°å¢ƒå¤‰æ•°TZãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            if os.getenv('TZ') == 'Asia/Tokyo':
                # TZç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°datetime.now()ã¯JSTã«ãªã‚‹
                today = datetime.now().strftime("%Y%m%d")
                timezone_info = "JST(TZ env)"
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: UTC+9æ™‚é–“ã§è¨ˆç®—
                today = (datetime.now() + timedelta(hours=9)).strftime("%Y%m%d")
                timezone_info = "JST(UTC+9)"
        
        filename = f"{today}.md"
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            # è¨˜äº‹å†…å®¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned_content = self._clean_article_content(content)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(cleaned_content)
            
            print(f"âœ… è¨˜äº‹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath} ({timezone_info}: {today})")
            return filepath
            
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _clean_article_content(self, content: str) -> str:
        """è¨˜äº‹å†…å®¹ã®é‡è¤‡ãƒ»ä¸è¦éƒ¨åˆ†ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        lines = content.split('\n')
        cleaned_lines = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’é™¤å»
            if line.startswith('ã‚¿ã‚¤ãƒˆãƒ«:') and i > 0:
                print(f"ğŸ§¹ é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’é™¤å»: {line[:50]}...")
                continue
                
            # ç©ºã®Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»
            if line == '```' and i < len(lines) - 1:
                next_line = lines[i + 1].strip()
                if next_line.startswith('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:') or next_line.startswith('blockquote:'):
                    print("ğŸ§¹ ä¸è¦ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»")
                    continue
                    
            # Claude APIã®å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ®‹ã‚Šã‚’é™¤å»
            if any(line.startswith(prefix) for prefix in [
                'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:', 'blockquote:', 'ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°:', 
                '```', 'ã‚¿ã‚¤ãƒˆãƒ«:', 'å‡ºåŠ›å½¢å¼', 'æ³¨æ„äº‹é …'
            ]) and i > 3:  # è¨˜äº‹é–‹å§‹å¾Œã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡ç¤ºã¯é™¤å»
                print(f"ğŸ§¹ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡ç¤ºã‚’é™¤å»: {line[:30]}...")
                continue
                
            cleaned_lines.append(line if line else '')
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
        final_lines = []
        prev_empty = False
        
        for line in cleaned_lines:
            if line == '':
                if not prev_empty:
                    final_lines.append(line)
                prev_empty = True
            else:
                final_lines.append(line)
                prev_empty = False
        
        cleaned_content = '\n'.join(final_lines).strip()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯: è¨˜äº‹ãŒæ­£å¸¸ãªæ§‹é€ ã‹ç¢ºèª
        if cleaned_content.startswith('#') and '>' in cleaned_content:
            print("âœ… è¨˜äº‹æ§‹é€ ã®å¦¥å½“æ€§ã‚’ç¢ºèªã—ã¾ã—ãŸ")
        else:
            print("âš ï¸ è¨˜äº‹æ§‹é€ ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        return cleaned_content
    
    async def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†"""
        print("ğŸš€ Peaky Media ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒªã‚µãƒ¼ãƒè¨˜äº‹ã¾ã¨ã‚ç”Ÿæˆé–‹å§‹")
        print("ğŸ¯ Product Researchã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’å¯¾è±¡ã«ã—ã¾ã™")
        print("=" * 60)
        
        try:
            # 1. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰Product Researchè¨˜äº‹ã®ã¿å–å¾—
            articles = self.fetch_peaky_articles()
            
            if not articles:
                print("âŒ Product Researchè¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                print("ğŸ’¡ Columnè¨˜äº‹ã¯é™¤å¤–ã•ã‚Œã¦ã„ã¾ã™")
                return
            
            # 2. ä¸Šä½è¨˜äº‹ã‚’é¸åˆ¥
            selected_articles = self.select_top_articles(articles, 5)
            
            if len(selected_articles) < 3:
                print("âš ï¸ ååˆ†ãªProduct Researchè¨˜äº‹æ•°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                print("ğŸ’¡ è¨˜äº‹æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                return
            
            # 3. Claude APIã§è¨˜äº‹ç”Ÿæˆ
            article_content = await self.generate_article_with_claude(selected_articles)
            
            # 4. ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            saved_path = self.save_article(article_content)
            
            if saved_path:
                print(f"ğŸ‰ ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆè¨˜äº‹ç”Ÿæˆå®Œäº†ï¼")
                print(f"ğŸ“ {saved_path}")
                print(f"ğŸ“Š {len(selected_articles)}ã¤ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚’å³é¸")
                print(f"âœ… Product Researchã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’å¯¾è±¡ã«ç”Ÿæˆ")
                
                print(f"\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
                print(f"   1. å†…å®¹ç¢ºèª: cat {saved_path}")
                print(f"   2. NoteæŠ•ç¨¿: python main.py")
            else:
                print("âŒ è¨˜äº‹ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")

async def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        generator = PeakyArticleGenerator()
        await generator.run()
    except ValueError as e:
        print(f"âŒ è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ’¡ .envãƒ•ã‚¡ã‚¤ãƒ«ã«ANTHROPIC_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    asyncio.run(main())